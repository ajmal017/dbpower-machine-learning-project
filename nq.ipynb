{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import talib as talib\n",
    "import numpy as np\n",
    "import data as ds\n",
    "import common as common\n",
    "import os as os\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "# 不让程序占满 GPU 内存\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = os.path.abspath(os.path.join('data', 'nq', '20201020_064100.csv'))\n",
    "file_2 = os.path.abspath(os.path.join('data', 'nq', '20201023_064200.csv'))\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chng obtains 0\n",
      "pchng obtains 0\n"
     ]
    }
   ],
   "source": [
    "# 1.0 reshapre dataframe 重構數據集\n",
    "df1.index = pd.to_datetime(df1.stime)\n",
    "df2.index = pd.to_datetime(df2.stime)\n",
    "df3 = pd.concat([df1, df2], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "df3 = df3.rename(columns={\"high\": \"High\", \"low\": \"Low\", \"open\": \"Open\", \"last\": \"Close\", \"vol\": \"Accumulated Volume\"})\n",
    "df3['udate'] = pd.to_datetime(df3['udate'])\n",
    "df3['udate2'] = [v.date() for v in df3['udate'].tolist()]\n",
    "df3['udate2'] = pd.to_datetime(df3['udate2'])\n",
    "df3['interactive_udate'] = pd.to_datetime(df3['interactive_udate'])\n",
    "df3['mdate'] = pd.to_datetime(df3['mdate'])\n",
    "df3['stime'] = pd.to_datetime(df3['stime'])\n",
    "# 1.1 data validation 數據有效性檢查\n",
    "shape = df3.shape\n",
    "types = df3.dtypes\n",
    "types1 = {'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Accumulated Volume': 'int64', 'chng': 'float64', 'pchng': 'float64'}\n",
    "df3.astype(types1).dtypes\n",
    "# 1.2 zero / nan\n",
    "for k, v in types1.items():\n",
    "    if (df3[k].isin([np.nan]).any().any()):\n",
    "        print(k+' obtains nan')\n",
    "    if (df3[k].isin([0]).any().any()):\n",
    "        print(k+' obtains 0')\n",
    "# 1.3 overview\n",
    "is_contain_nan = df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 technical indicator\n",
    "highs = np.array(df3['High'], dtype='float')\n",
    "lows = np.array(df3['Low'], dtype='float')\n",
    "opens = np.array(df3['Open'], dtype='float')\n",
    "closes = np.array(df3['Close'], dtype='float')\n",
    "# 2.1 SMA 均線\n",
    "for v in [5, 10, 20, 50, 80, 100, 120, 150, 180, 200]:\n",
    "    df3['sma-'+str(v)]= talib.SMA(closes, timeperiod=v)\n",
    "# 2.2 Bollinger 保力加\n",
    "df3['upper-band'], df3['middle-band'], df3['lower-band'] = talib.BBANDS(closes, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "# 2.3 %B %保力加\n",
    "df3['%b'] = (df3['Close']-df3['lower-band'])/(df3['upper-band']-df3['lower-band'])*100\n",
    "# 2.4 P-SAR 抛物线\n",
    "df3['p-sar'] = talib.SAR(highs, lows, acceleration=0.02, maximum=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 separate to daily data set\n",
    "data = {}\n",
    "days1 = list(dict.fromkeys(df3['udate2'].tolist())) # how many tradeing day\n",
    "for day in days1:\n",
    "    day_start = datetime(day.year, day.month, day.day, 6, 0, 0)\n",
    "    day2 = day + timedelta(days=1)\n",
    "    day_end = datetime(day2.year, day2.month, day2.day, 6, 0, 0)\n",
    "    mask = ((df3['udate'] >= day_start) & (df3['udate'] <= day_end))\n",
    "    df4 = df3.loc[mask]\n",
    "    if (df4.shape[0] > 1):\n",
    "        # 3.1 calculate vol by minute\n",
    "        df4['Volume'] = df4['Accumulated Volume'].diff()\n",
    "        # 3.2 OBV 能量潮\n",
    "        closes = np.array(df4['Close'], dtype='float')\n",
    "        vols = np.array(df4['Volume'], dtype='float')\n",
    "        df4['obv'] = talib.OBV(closes, vols)\n",
    "        # 3.3 vol EMA\n",
    "        df4['vol-sma5'] = talib.EMA(vols, timeperiod=5)\n",
    "        # 3.4 %b\n",
    "        df4['%b-high']  = common.percentB_belowzero(df4['%b'], df4['Close']) \n",
    "        df4['%b-low'] = common.percentB_aboveone(df4['%b'], df4['Close'])\n",
    "        # 3.5 assgin to data\n",
    "        data[day] = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 draw chart\n",
    "for k, df5 in data.items():\n",
    "    # 4.1 style\n",
    "    style = mpf.make_mpf_style(base_mpf_style='charles', rc={'font.size':6})\n",
    "    # 4.2 addplot\n",
    "    apds = [mpf.make_addplot(df5[['lower-band','upper-band']],panel=0,color='orange',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['%b-low'],type='scatter',markersize=20,marker='v'),\n",
    "            mpf.make_addplot(df5['%b-high'],type='scatter',markersize=20,marker='^'),\n",
    "            mpf.make_addplot(df5['p-sar'],scatter=True,markersize=1,marker='*',panel=0,color='blueviolet'),\n",
    "            mpf.make_addplot((df5['vol-sma5']),panel=1,color='orange')]\n",
    "    # 4.3 render\n",
    "    if (False):\n",
    "        print(df5.shape, df5['udate'].iloc[0], df5['udate'].iloc[-1])\n",
    "        mpf.plot(df5, type='line', style=style, ylabel='', ylabel_lower='', volume=True, figscale=0.5, xrotation=0, datetime_format=\"%H:%M\", show_nontrading=False, tight_layout=True, addplot=apds) # savefig='./data/img-nq/'+k.strftime('%m-%d-%Y') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset has 13268 samples, and 26 features.\n",
      "Train Data: (12744, 26), Test Data: (523, 26)\n",
      "X_train Data: (12744, 26, 1), Y_train Data: (12744,)\n",
      "X_Test Data: (523, 26, 1), Y_Test Data: (523,)\n"
     ]
    }
   ],
   "source": [
    "# 5.0 合併\n",
    "df7 = pd.DataFrame()\n",
    "for k, df6 in data.items():\n",
    "    df7 = pd.concat([df6, df7], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "# 5.1 清洗\n",
    "df8 = df7.copy(deep=True)\n",
    "drop_list = ['udate', 'code', 'name', 'nmll', 'bid', 'ask', 'bsize', 'asize', 'clast', 'Accumulated Volume', 'turnover', \n",
    "             'currency',  'yrhigh', 'yrlow', 'tnover_sc', 'lotsize', 'calllv', 'issued', 'ichng', 'ucode', 'ratio', 'strike', 'mdate', \n",
    "             'issuer', 'traded', 'ocode', 'eikon_udate', 'interactive_udate', 'instrument_type', 'stime', 'udate2']\n",
    "df8.drop(drop_list, axis=1, inplace=True)\n",
    "df8.fillna(0, inplace=True)\n",
    "is_contain_null = df8.isnull().sum()\n",
    "is_contain_nan = df8.isna().sum()\n",
    "print('Total dataset has {} samples, and {} features.'.format(df8.shape[0], df8.shape[1])) # df8.info()\n",
    "# 5.2 分包\n",
    "train_data = df8.iloc[0: 12744]\n",
    "test_data = df8.iloc[12745: 13268]\n",
    "print('Train Data: {}, Test Data: {}'.format(train_data.shape, test_data.shape))\n",
    "# 5.3 模型数据\n",
    "# 5.3.1 train data\n",
    "x_train, y_train = [], []\n",
    "for k, v in train_data.iterrows():\n",
    "    x_train.append(v.tolist())\n",
    "    y_train.append(v['Close'])\n",
    "x_train, y_train =  np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],  x_train.shape[1],  1))\n",
    "print('X_train Data: {}, Y_train Data: {}'.format(x_train.shape, y_train.shape))\n",
    "# 5.3.2 test data\n",
    "x_test, y_test = [], []\n",
    "for k, v in test_data.iterrows():\n",
    "    x_test.append(v.tolist())\n",
    "    y_test.append(v['Close'])\n",
    "x_test, y_test =  np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],  x_test.shape[1],  1))\n",
    "print('X_Test Data: {}, Y_Test Data: {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 26, 1000)          4008000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 26, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 26, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 26, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26, 1)             1001      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 26, 1)             0         \n",
      "=================================================================\n",
      "Total params: 28,021,001\n",
      "Trainable params: 28,021,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 15s 38ms/step - loss: -74410.6484 - accuracy: 0.0161\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 15s 38ms/step - loss: -75064.0938 - accuracy: 0.0160\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 15s 38ms/step - loss: -74931.9688 - accuracy: 0.0157\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -74783.6562 - accuracy: 0.0159\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -75162.9531 - accuracy: 0.0160\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -74872.8672 - accuracy: 0.0161\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -74749.7969 - accuracy: 0.0162\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -74673.1797 - accuracy: 0.0158\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -75025.8281 - accuracy: 0.0161\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 15s 39ms/step - loss: -75207.8906 - accuracy: 0.0160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff7ad672990>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.0\n",
    "batch_size = None\n",
    "num_steps = None\n",
    "hidden_size = None\n",
    "# 6.1 模型\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=1000, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=1000, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=1000, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=1000, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.Dense(units=1))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "# 6.1 fit model\n",
    "model_lstm.fit(x_train, x_train, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
