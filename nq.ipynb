{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import talib as talib\n",
    "import numpy as np\n",
    "import data as ds\n",
    "import common as common\n",
    "import os as os\n",
    "import datetime as datetime\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设定\n",
    "pd.options.mode.chained_assignment = None\n",
    "# 不让程序占满 GPU 内存\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据源 v1\n",
    "file_1 = os.path.abspath(os.path.join('data', 'nq', '20201020_064100.csv'))\n",
    "file_2 = os.path.abspath(os.path.join('data', 'nq', '20201023_064200.csv'))\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)\n",
    "\n",
    "# 数据源 v2\n",
    "file_3 = os.path.abspath(os.path.join('data', 'nq', 'nq-20201029.csv'))\n",
    "_df1 = pd.read_csv(file_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume obtains 0\n"
     ]
    }
   ],
   "source": [
    "# 1.0 reshapre dataframe v1 重構數據集\n",
    "df1.index = pd.to_datetime(df1.stime)\n",
    "df2.index = pd.to_datetime(df2.stime)\n",
    "df3 = pd.concat([df1, df2], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "df3 = df3.rename(columns={\"high\": \"High\", \"low\": \"Low\", \"open\": \"Open\", \"last\": \"Close\", \"vol\": \"Accumulated Volume\"})\n",
    "df3['udate'] = pd.to_datetime(df3['udate'])\n",
    "df3['interactive_udate'] = pd.to_datetime(df3['interactive_udate'])\n",
    "df3['mdate'] = pd.to_datetime(df3['mdate'])\n",
    "df3['stime'] = pd.to_datetime(df3['stime'])\n",
    "shape = df3.shape\n",
    "types = df3.dtypes\n",
    "types1 = {'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Accumulated Volume': 'int64', 'chng': 'float64', 'pchng': 'float64'}\n",
    "df3.astype(types1).dtypes\n",
    "\n",
    "# 1.0 reshapre dataframe v2 重構數據集\n",
    "_df1.columns = ['udate', 'High', 'Low', 'Open', 'Close', 'Volume']\n",
    "types2 = {'udate': 'object', 'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Volume': 'int64'}\n",
    "_df1.astype(types2).dtypes\n",
    "_df2 = _df1.copy(deep=True)\n",
    "error_row = []\n",
    "for k, v in _df1.iterrows():\n",
    "    if not pd.isnull(_df1['udate'].iloc[k]):\n",
    "        stime = str(int(_df1['udate'].iloc[k]))\n",
    "        _df2['udate'].iloc[k] = datetime(year=2020, month=int(stime[-8:-6]), day=int(stime[-6:-4]), hour=int(stime[-4:-2]), minute=int(stime[-2:]), second=0)\n",
    "    else:\n",
    "        error_row.append(k)\n",
    "_df2.drop(_df2.index[error_row], inplace=True)\n",
    "_df2.udate = pd.to_datetime(_df2.udate)\n",
    "_df2.index = pd.to_datetime(_df2.udate)\n",
    "df3 = _df2.copy(deep=True)\n",
    "\n",
    "# 1.2 zero / nan 數據有效性檢查\n",
    "for k, v in types2.items():\n",
    "    if (df3[k].isin([np.nan]).any().any()):\n",
    "        print(k+' obtains nan')\n",
    "    if (df3[k].isin([0]).any().any()):\n",
    "        print(k+' obtains 0')\n",
    "# 1.3 overview\n",
    "is_contain_nan = df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 technical indicator\n",
    "highs = np.array(df3['High'], dtype='float')\n",
    "lows = np.array(df3['Low'], dtype='float')\n",
    "opens = np.array(df3['Open'], dtype='float')\n",
    "closes = np.array(df3['Close'], dtype='float')\n",
    "vols = np.array(df3['Volume'], dtype='float')\n",
    "# 2.1 SMA 均線\n",
    "for v in [5, 10, 20, 50, 80, 100, 120, 150, 180, 200]:\n",
    "    df3['sma-'+str(v)]= talib.SMA(closes, timeperiod=v)\n",
    "# 2.2 Bollinger 保力加\n",
    "df3['upper-band'], df3['middle-band'], df3['lower-band'] = talib.BBANDS(closes, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "# 2.3 %B %保力加\n",
    "df3['%b'] = (df3['Close']-df3['lower-band'])/(df3['upper-band']-df3['lower-band'])*100\n",
    "df3['%b-high']  = common.percentB_belowzero(df3['%b'], df3['Close']) \n",
    "df3['%b-low'] = common.percentB_aboveone(df3['%b'], df3['Close'])\n",
    "# 3.2 OBV 能量潮\n",
    "df3['obv'] = talib.OBV(closes, vols)\n",
    "# 3.3 vol EMA\n",
    "df3['vol-ema5'] = talib.EMA(vols, timeperiod=5)\n",
    "# 2.4 P-SAR 抛物线\n",
    "df3['p-sar'] = talib.SAR(highs, lows, acceleration=0.02, maximum=0.2)\n",
    "# 2.5 vwap 成交量加權平均價格\n",
    "min_max_scaler, standar_scaler = MinMaxScaler(), StandardScaler()\n",
    "period, vwap, vwap2 = [5, 20, 50, 100], [], []\n",
    "for v in period:\n",
    "    df3['typical-price'] = (df3['High'] + df3['Low'] + df3['Close']) / 3\n",
    "    df3['turnover'] = df3['typical-price'] * df3['Volume']\n",
    "    df3['cum-turnover-'+str(v)] = df3['turnover'].rolling(window=v).sum()\n",
    "    df3['cum-volume-'+str(v)] = df3['Volume'].rolling(window=v).sum()\n",
    "    df3['vwap-'+str(v)] = df3['cum-turnover-'+str(v)]  /  df3['cum-volume-'+str(v)]\n",
    "    df3['vwap-'+str(v)] = df3['vwap-'+str(v)].replace([np.inf, -np.inf], 0)\n",
    "    df3['vwap-'+str(v)].fillna(0, inplace=True)\n",
    "    drop_list = ['turnover', 'typical-price', 'cum-turnover-'+str(v), 'cum-volume-'+str(v)]\n",
    "    df3.drop(drop_list, axis=1, inplace=True)\n",
    "    vwap.append('vwap-'+str(v))\n",
    "    vwap2.append('vwap-nor-'+str(v))\n",
    "df3[vwap2] = standar_scaler.fit_transform(df3[vwap].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 separate to daily data set\n",
    "data = {}\n",
    "days1 = list(dict.fromkeys([v.date() for v in df3['udate']])) # how many tradeing day\n",
    "for day in days1:\n",
    "    day_start = datetime(day.year, day.month, day.day, 6, 0, 0)\n",
    "    day2 = day + timedelta(days=1)\n",
    "    day_end = datetime(day2.year, day2.month, day2.day, 6, 0, 0)\n",
    "    mask = ((df3['udate'] >= day_start) & (df3['udate'] <= day_end))\n",
    "    df4 = df3.loc[mask]\n",
    "    if (df4.shape[0] > 1):\n",
    "        data[day] = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 draw chart\n",
    "for k, df5 in data.items():\n",
    "    # 4.1 style\n",
    "    style = mpf.make_mpf_style(base_mpf_style='charles', rc={'font.size':6})\n",
    "    # 4.2 addplot\n",
    "    apds = [mpf.make_addplot(df5[['lower-band','upper-band']],panel=0,color='orange',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['vwap-20'].replace(0, np.nan),panel=0,color='aqua',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['%b-low'],type='scatter',markersize=20,marker='v'),\n",
    "            mpf.make_addplot(df5['%b-high'],type='scatter',markersize=20,marker='^'),\n",
    "            mpf.make_addplot(df5['p-sar'],scatter=True,markersize=1,marker='*',panel=0,color='blueviolet'),\n",
    "            mpf.make_addplot((df5['vol-ema5']),panel=1,color='orange')]\n",
    "    # 4.3 render\n",
    "    if (False):\n",
    "        # print(df5.shape, df5['udate'].iloc[0], df5['udate'].iloc[-1])\n",
    "        mpf.plot(df5, type='candle', addplot=apds, style=style, ylabel='', ylabel_lower='', volume=True, figscale=1.25, xrotation=0, datetime_format=\"%H:%M\", show_nontrading=False, tight_layout=True, savefig='./data/img-nq/'+k.strftime('%m-%d-%Y')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集: Total dataset has 28600 samples, and 24 features.\n",
      "分包: Train Data: (26008, 24), Test Data: (2392, 24)\n",
      "訓練集: X_train Data: (26008, 24, 1), Y_train Data: (26008,)\n",
      "測試集: X_Test Data: (2392, 24, 1), Y_Test Data: (2392,)\n"
     ]
    }
   ],
   "source": [
    "# 5.0 合併\n",
    "df7 = pd.DataFrame()\n",
    "for k, df6 in data.items():\n",
    "    df7 = pd.concat([df7, df6], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "# 5.1.1 清洗\n",
    "df8 = df7.copy(deep=True)\n",
    "drop_list = ['udate', 'High', 'Low', 'Open', 'Volume']+vwap # drop_list = ['udate', 'code', 'name', 'nmll', 'bid', 'ask', 'bsize', 'asize', 'clast', 'Accumulated Volume', 'turnover', 'currency',  'yrhigh', 'yrlow', 'tnover_sc', 'lotsize', 'calllv', 'issued', 'ichng', 'ucode', 'ratio', 'strike', 'mdate', 'issuer', 'traded', 'ocode', 'eikon_udate', 'interactive_udate', 'instrument_type', 'stime', 'udate2']\n",
    "df8.drop(drop_list, axis=1, inplace=True)\n",
    "df8.fillna(0, inplace=True)\n",
    "is_contain_null = df8.isnull().sum()\n",
    "is_contain_nan = df8.isna().sum()\n",
    "print('数据集: Total dataset has {} samples, and {} features.'.format(df8.shape[0], df8.shape[1])) # df8.info()\n",
    "# 5.1.2 save all data to csv\n",
    "path_data = os.path.abspath(os.path.join('data', 'nq', 'nq-clean-data-with-features.csv'))\n",
    "if os.path.exists(path_data):\n",
    "    os.remove(path_data)\n",
    "df8.to_csv(path_data)\n",
    "# 5.2 分包\n",
    "no_day = np.array([v.shape[0] for k, v in data.items()])\n",
    "no_day_1, no_day_2 = no_day[:-2].sum(), no_day.sum()\n",
    "train_data = df8.iloc[200 : no_day_1]\n",
    "test_data = df8.iloc[no_day_1 : no_day_2]\n",
    "print('分包: Train Data: {}, Test Data: {}'.format(train_data.shape, test_data.shape))\n",
    "# 5.3 模型数据\n",
    "# 5.3.1 train data\n",
    "x_train, y_train = [], []\n",
    "for k, v in train_data.iterrows():\n",
    "    x_train.append(v.tolist())\n",
    "    y_train.append(v['Close'])\n",
    "x_train, y_train =  np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],  x_train.shape[1],  1))\n",
    "print('訓練集: X_train Data: {}, Y_train Data: {}'.format(x_train.shape, y_train.shape))\n",
    "# 5.3.2 test data\n",
    "x_test, y_test = [], []\n",
    "for k, v in test_data.iterrows():\n",
    "    x_test.append(v.tolist())\n",
    "    y_test.append(v['Close'])\n",
    "x_test, y_test =  np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],  x_test.shape[1],  1))\n",
    "print('測試集: X_Test Data: {}, Y_Test Data: {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_128 (LSTM)              (None, 24, 1000)          4008000   \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_129 (LSTM)              (None, 24, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_130 (LSTM)              (None, 24, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_131 (LSTM)              (None, 24, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 24, 1)             1001      \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 24, 1)             0         \n",
      "=================================================================\n",
      "Total params: 28,021,001\n",
      "Trainable params: 28,021,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "2168/2168 [==============================] - 79s 36ms/step - loss: -75178.7266 - accuracy: 0.0164 - mae: 9044.8135 - sparse_categorical_accuracy: 0.0818 - val_loss: -174163.8750 - val_accuracy: 0.0000e+00 - val_mae: 11405.9424 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2168/2168 [==============================] - 78s 36ms/step - loss: -75538.4297 - accuracy: 0.0163 - mae: 9044.7861 - sparse_categorical_accuracy: 0.0818 - val_loss: -174163.8750 - val_accuracy: 0.0000e+00 - val_mae: 11405.9424 - val_sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# 6.0.1 模型参数\n",
    "batch_size = 12\n",
    "epochs = 2  # 80\n",
    "units = 1000 # 1000\n",
    "# 6.0.2 日志参数\n",
    "prefix = 'nq-lstm'\n",
    "cur_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 6.1 模型\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.Dense(units=1))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'mae', 'sparse_categorical_accuracy'])\n",
    "model_lstm.summary()\n",
    "\n",
    "# 6.2.1 創建check point\n",
    "checkpoint_dir = './training_checkpoints/'+ prefix +'-' + cur_time\n",
    "os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "# 6.2.2 創建tensor board\n",
    "log_dir = os.path.join('./logs/fit/'+ prefix +'-') + cur_time\n",
    "os.mkdir(log_dir)\n",
    "tensor_board_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# 6.3 fit model\n",
    "history_lstm = model_lstm.fit(x_train, x_train, epochs = epochs, batch_size = batch_size, \n",
    "                              validation_data = (x_test, y_test),\n",
    "                              callbacks=[checkpoint_callback, tensor_board_callback])\n",
    "\n",
    "# 6.4 儲存模型\n",
    "model_path = \"./saved_model/nq_lstm_\"+cur_time+\".h5\"\n",
    "model_lstm.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
