{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import talib as talib\n",
    "import numpy as np\n",
    "import data as ds\n",
    "import common as common\n",
    "import os as os\n",
    "import datetime as datetime\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设定\n",
    "pd.options.mode.chained_assignment = None\n",
    "# 不让程序占满 GPU 内存\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据源 v1\n",
    "file_1 = os.path.abspath(os.path.join('data', 'nq', '20201020_064100.csv'))\n",
    "file_2 = os.path.abspath(os.path.join('data', 'nq', '20201023_064200.csv'))\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)\n",
    "\n",
    "# 数据源 v2\n",
    "file_3 = os.path.abspath(os.path.join('data', 'nq', 'nq-20201029.csv'))\n",
    "_df1 = pd.read_csv(file_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume obtains 0\n"
     ]
    }
   ],
   "source": [
    "# 1.0 reshapre dataframe v1 重構數據集\n",
    "df1.index = pd.to_datetime(df1.stime)\n",
    "df2.index = pd.to_datetime(df2.stime)\n",
    "df3 = pd.concat([df1, df2], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "df3 = df3.rename(columns={\"high\": \"High\", \"low\": \"Low\", \"open\": \"Open\", \"last\": \"Close\", \"vol\": \"Accumulated Volume\"})\n",
    "df3['udate'] = pd.to_datetime(df3['udate'])\n",
    "df3['interactive_udate'] = pd.to_datetime(df3['interactive_udate'])\n",
    "df3['mdate'] = pd.to_datetime(df3['mdate'])\n",
    "df3['stime'] = pd.to_datetime(df3['stime'])\n",
    "shape = df3.shape\n",
    "types = df3.dtypes\n",
    "types1 = {'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Accumulated Volume': 'int64', 'chng': 'float64', 'pchng': 'float64'}\n",
    "df3.astype(types1).dtypes\n",
    "\n",
    "# 1.0 reshapre dataframe v2 重構數據集\n",
    "_df1.columns = ['udate', 'High', 'Low', 'Open', 'Close', 'Volume']\n",
    "types2 = {'udate': 'object', 'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Volume': 'int64'}\n",
    "_df1.astype(types2).dtypes\n",
    "_df2 = _df1.copy(deep=True)\n",
    "error_row = []\n",
    "for k, v in _df1.iterrows():\n",
    "    if not pd.isnull(_df1['udate'].iloc[k]):\n",
    "        stime = str(int(_df1['udate'].iloc[k]))\n",
    "        _df2['udate'].iloc[k] = datetime(year=2020, month=int(stime[-8:-6]), day=int(stime[-6:-4]), hour=int(stime[-4:-2]), minute=int(stime[-2:]), second=0)\n",
    "    else:\n",
    "        error_row.append(k)\n",
    "_df2.drop(_df2.index[error_row], inplace=True)\n",
    "_df2.udate = pd.to_datetime(_df2.udate)\n",
    "_df2.index = pd.to_datetime(_df2.udate)\n",
    "df3 = _df2.copy(deep=True)\n",
    "\n",
    "# 1.2 zero / nan 數據有效性檢查\n",
    "for k, v in types2.items():\n",
    "    if (df3[k].isin([np.nan]).any().any()):\n",
    "        print(k+' obtains nan')\n",
    "    if (df3[k].isin([0]).any().any()):\n",
    "        print(k+' obtains 0')\n",
    "# 1.3 overview\n",
    "is_contain_nan = df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 technical indicator\n",
    "highs = np.array(df3['High'], dtype='float')\n",
    "lows = np.array(df3['Low'], dtype='float')\n",
    "opens = np.array(df3['Open'], dtype='float')\n",
    "closes = np.array(df3['Close'], dtype='float')\n",
    "# 2.1 SMA 均線\n",
    "for v in [5, 10, 20, 50, 80, 100, 120, 150, 180, 200]:\n",
    "    df3['sma-'+str(v)]= talib.SMA(closes, timeperiod=v)\n",
    "# 2.2 Bollinger 保力加\n",
    "df3['upper-band'], df3['middle-band'], df3['lower-band'] = talib.BBANDS(closes, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "# 2.3 %B %保力加\n",
    "df3['%b'] = (df3['Close']-df3['lower-band'])/(df3['upper-band']-df3['lower-band'])*100\n",
    "# 2.4 P-SAR 抛物线\n",
    "df3['p-sar'] = talib.SAR(highs, lows, acceleration=0.02, maximum=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 separate to daily data set\n",
    "data = {}\n",
    "days1 = list(dict.fromkeys([v.date() for v in df3['udate']])) # how many tradeing day\n",
    "for day in days1:\n",
    "    day_start = datetime(day.year, day.month, day.day, 6, 0, 0)\n",
    "    day2 = day + timedelta(days=1)\n",
    "    day_end = datetime(day2.year, day2.month, day2.day, 6, 0, 0)\n",
    "    mask = ((df3['udate'] >= day_start) & (df3['udate'] <= day_end))\n",
    "    df4 = df3.loc[mask]\n",
    "    if (df4.shape[0] > 1):\n",
    "        # 3.1 calculate vol by minute\n",
    "        # df4['Volume'] = df4['Accumulated Volume'].diff()\n",
    "        # 3.2 OBV 能量潮\n",
    "        closes = np.array(df4['Close'], dtype='float')\n",
    "        vols = np.array(df4['Volume'], dtype='float')\n",
    "        df4['obv'] = talib.OBV(closes, vols)\n",
    "        # 3.3 vol EMA\n",
    "        df4['vol-sma5'] = talib.EMA(vols, timeperiod=5)\n",
    "        # 3.4 %b\n",
    "        df4['%b-high']  = common.percentB_belowzero(df4['%b'], df4['Close']) \n",
    "        df4['%b-low'] = common.percentB_aboveone(df4['%b'], df4['Close'])\n",
    "        # 3.5 assgin to data\n",
    "        data[day] = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780, 25) 2020-09-30 17:01:00 2020-10-01 06:00:00\n",
      "(1426, 25) 2020-10-01 06:00:00 2020-10-02 06:00:00\n",
      "(585, 25) 2020-10-02 06:00:00 2020-10-02 15:59:00\n",
      "(781, 25) 2020-10-04 17:00:00 2020-10-05 06:00:00\n",
      "(1426, 25) 2020-10-05 06:00:00 2020-10-06 06:00:00\n",
      "(1426, 25) 2020-10-06 06:00:00 2020-10-07 06:00:00\n",
      "(1426, 25) 2020-10-07 06:00:00 2020-10-08 06:00:00\n",
      "(1426, 25) 2020-10-08 06:00:00 2020-10-09 06:00:00\n",
      "(585, 25) 2020-10-09 06:00:00 2020-10-09 15:59:00\n",
      "(781, 25) 2020-10-11 17:00:00 2020-10-12 06:00:00\n",
      "(1426, 25) 2020-10-12 06:00:00 2020-10-13 06:00:00\n",
      "(1426, 25) 2020-10-13 06:00:00 2020-10-14 06:00:00\n",
      "(1426, 25) 2020-10-14 06:00:00 2020-10-15 06:00:00\n",
      "(1426, 25) 2020-10-15 06:00:00 2020-10-16 06:00:00\n",
      "(585, 25) 2020-10-16 06:00:00 2020-10-16 15:59:00\n",
      "(781, 25) 2020-10-18 17:00:00 2020-10-19 06:00:00\n",
      "(1426, 25) 2020-10-19 06:00:00 2020-10-20 06:00:00\n",
      "(1426, 25) 2020-10-20 06:00:00 2020-10-21 06:00:00\n",
      "(1426, 25) 2020-10-21 06:00:00 2020-10-22 06:00:00\n",
      "(1426, 25) 2020-10-22 06:00:00 2020-10-23 06:00:00\n",
      "(585, 25) 2020-10-23 06:00:00 2020-10-23 15:59:00\n",
      "(781, 25) 2020-10-25 17:00:00 2020-10-26 06:00:00\n",
      "(1426, 25) 2020-10-26 06:00:00 2020-10-27 06:00:00\n",
      "(1426, 25) 2020-10-27 06:00:00 2020-10-28 06:00:00\n",
      "(966, 25) 2020-10-28 06:00:00 2020-10-28 22:20:00\n"
     ]
    }
   ],
   "source": [
    "# 4.0 draw chart\n",
    "for k, df5 in data.items():\n",
    "    # 4.1 style\n",
    "    style = mpf.make_mpf_style(base_mpf_style='charles', rc={'font.size':6})\n",
    "    # 4.2 addplot\n",
    "    apds = [mpf.make_addplot(df5[['lower-band','upper-band']],panel=0,color='orange',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['%b-low'],type='scatter',markersize=20,marker='v'),\n",
    "            mpf.make_addplot(df5['%b-high'],type='scatter',markersize=20,marker='^'),\n",
    "            mpf.make_addplot(df5['p-sar'],scatter=True,markersize=1,marker='*',panel=0,color='blueviolet'),\n",
    "            mpf.make_addplot((df5['vol-sma5']),panel=1,color='orange')]\n",
    "    # 4.3 render\n",
    "    if (True):\n",
    "        print(df5.shape, df5['udate'].iloc[0], df5['udate'].iloc[-1])\n",
    "        mpf.plot(df5, type='candle', addplot=apds, style=style, ylabel='', ylabel_lower='', volume=True, figscale=1, xrotation=0, datetime_format=\"%H:%M\", show_nontrading=False, tight_layout=True, savefig='./data/img-nq/'+k.strftime('%m-%d-%Y')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset has 28600 samples, and 24 features.\n",
      "Train Data: (12744, 24), Test Data: (523, 24)\n",
      "X_train Data: (12744, 24, 1), Y_train Data: (12744,)\n",
      "X_Test Data: (523, 24, 1), Y_Test Data: (523,)\n"
     ]
    }
   ],
   "source": [
    "# 5.0 合併\n",
    "df7 = pd.DataFrame()\n",
    "for k, df6 in data.items():\n",
    "    df7 = pd.concat([df6, df7], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "# 5.1 清洗\n",
    "df8 = df7.copy(deep=True)\n",
    "\"\"\"drop_list = ['udate', 'code', 'name', 'nmll', 'bid', 'ask', 'bsize', 'asize', 'clast', 'Accumulated Volume', 'turnover', \n",
    "             'currency',  'yrhigh', 'yrlow', 'tnover_sc', 'lotsize', 'calllv', 'issued', 'ichng', 'ucode', 'ratio', 'strike', 'mdate', \n",
    "             'issuer', 'traded', 'ocode', 'eikon_udate', 'interactive_udate', 'instrument_type', 'stime', 'udate2']\"\"\"\n",
    "drop_list = ['udate']\n",
    "df8.drop(drop_list, axis=1, inplace=True)\n",
    "df8.fillna(0, inplace=True)\n",
    "is_contain_null = df8.isnull().sum()\n",
    "is_contain_nan = df8.isna().sum()\n",
    "print('Total dataset has {} samples, and {} features.'.format(df8.shape[0], df8.shape[1])) # df8.info()\n",
    "# 5.2 分包\n",
    "train_data = df8.iloc[0 : 12744]\n",
    "test_data = df8.iloc[12745 : 13268]\n",
    "print('Train Data: {}, Test Data: {}'.format(train_data.shape, test_data.shape))\n",
    "# 5.3 模型数据\n",
    "# 5.3.1 train data\n",
    "x_train, y_train = [], []\n",
    "for k, v in train_data.iterrows():\n",
    "    x_train.append(v.tolist())\n",
    "    y_train.append(v['Close'])\n",
    "x_train, y_train =  np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],  x_train.shape[1],  1))\n",
    "print('X_train Data: {}, Y_train Data: {}'.format(x_train.shape, y_train.shape))\n",
    "# 5.3.2 test data\n",
    "x_test, y_test = [], []\n",
    "for k, v in test_data.iterrows():\n",
    "    x_test.append(v.tolist())\n",
    "    y_test.append(v['Close'])\n",
    "x_test, y_test =  np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],  x_test.shape[1],  1))\n",
    "print('X_Test Data: {}, Y_Test Data: {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 24, 1000)          4008000   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 24, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 24, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 24, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24, 1)             1001      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 24, 1)             0         \n",
      "=================================================================\n",
      "Total params: 28,021,001\n",
      "Trainable params: 28,021,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -73457.4688 - accuracy: 0.0167\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -74017.1250 - accuracy: 0.0168\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -73671.0078 - accuracy: 0.0166\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -74206.0234 - accuracy: 0.0168\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -73567.5703 - accuracy: 0.0167\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 17s 42ms/step - loss: -74043.0547 - accuracy: 0.0168\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -74524.6719 - accuracy: 0.0167\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 17s 42ms/step - loss: -73787.8125 - accuracy: 0.0165\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 16s 41ms/step - loss: -74017.5703 - accuracy: 0.0167\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 17s 42ms/step - loss: -73714.5156 - accuracy: 0.0166\n"
     ]
    }
   ],
   "source": [
    "# 6.0 模型参数\n",
    "batch_size = 32\n",
    "epochs = 10 # 80\n",
    "units = 1000 # 1000\n",
    "cur_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 6.1 模型\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.Dense(units=1))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n",
    "# 6.2.1 創建check point\n",
    "checkpoint_dir = './training_checkpoints/nq-lstm-' + cur_time\n",
    "os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "# 6.2.2 創建tensor board\n",
    "log_dir = os.path.join('./logs/fit/nq-lstm-') + cur_time\n",
    "os.mkdir(log_dir)\n",
    "tensor_board_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# 6.3 fit model\n",
    "history = model_lstm.fit(x_train, x_train, epochs = epochs, batch_size = batch_size, \n",
    "                         callbacks=[checkpoint_callback, tensor_board_callback])\n",
    "\n",
    "# 6.4 儲存模型\n",
    "model_path = \"./saved_model/nq_lstm_\"+cur_time+\".h5\"\n",
    "model_lstm.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
