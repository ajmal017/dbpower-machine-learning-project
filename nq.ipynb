{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import talib as talib\n",
    "import numpy as np\n",
    "import data as ds\n",
    "import common as common\n",
    "import os as os\n",
    "import math as math\n",
    "import datetime as datetime\n",
    "from itertools import repeat\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 环境设定\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# 0.2 不让程序占满 GPU 内存\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume obtains 0\n",
      "Volume obtains 0\n"
     ]
    }
   ],
   "source": [
    "# 1.0 重構數據集\n",
    "def reshape_dataframe(df0):\n",
    "    df0.fillna(0, inplace=True)\n",
    "    df0.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df0.columns = ['udate', 'High', 'Low', 'Open', 'Close', 'Volume']\n",
    "    types2 = {'udate': 'object', 'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Volume': 'int64'}\n",
    "    df0.astype(types2).dtypes\n",
    "    df0_1 = df0.copy(deep=True)\n",
    "    error_row = []\n",
    "    for k, v in df0.iterrows():\n",
    "        if not pd.isnull(df0['udate'].iloc[k]) and df0['udate'].iloc[k] > 0:\n",
    "            stime = str(int(df0['udate'].iloc[k]))\n",
    "            df0_1['udate'].iloc[k] = datetime(year=2020, month=int(stime[-8:-6]), day=int(stime[-6:-4]), hour=int(stime[-4:-2]), minute=int(stime[-2:]), second=0)\n",
    "        else:\n",
    "            error_row.append(k)\n",
    "    df0_1.drop(df0_1.index[error_row], inplace=True)\n",
    "    df0_1.udate = pd.to_datetime(df0_1.udate)\n",
    "    df0_1.index = pd.to_datetime(df0_1.udate)\n",
    "    \n",
    "    # 1.0.1 數據有效性檢查\n",
    "    for k, v in types2.items():\n",
    "        if (df0_1[k].isin([np.nan]).any().any()):\n",
    "            print(k+' obtains nan')\n",
    "        if (df0_1[k].isin([0]).any().any()):\n",
    "            print(k+' obtains 0')\n",
    "    is_contain_null = df0_1.isnull().sum()\n",
    "    \n",
    "    # 1.0.2\n",
    "    return df0_1\n",
    "\n",
    "# 1.1 数据源\n",
    "file_1 = os.path.abspath(os.path.join('data', 'nq', 'nq-20201105.csv'))\n",
    "df1 = reshape_dataframe(pd.read_csv(file_1))\n",
    "file_2 = os.path.abspath(os.path.join('data', 'nq', 'nq-20201105_Aug_Sep.csv'))\n",
    "df2 = reshape_dataframe(pd.read_csv(file_2))\n",
    "df3 = pd.concat([df1, df2], ignore_index=False)\n",
    "\n",
    "# 1.2 刪除重覆index\n",
    "df3 = df3.groupby(df3.index).first()\n",
    "\n",
    "# 1.3 排序\n",
    "df3.sort_index(axis=0, ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 技術指標\n",
    "highs = np.array(df3['High'], dtype='float')\n",
    "lows = np.array(df3['Low'], dtype='float')\n",
    "opens = np.array(df3['Open'], dtype='float')\n",
    "closes = np.array(df3['Close'], dtype='float')\n",
    "vols = np.array(df3['Volume'], dtype='float')\n",
    "# 2.1 SMA 均線\n",
    "for v in [5, 40, 80, 120]:\n",
    "    df3['sma-'+str(v)]= talib.SMA(closes, timeperiod=v)\n",
    "# 2.2 Bollinger 保力加\n",
    "df3['upper-band'], df3['middle-band'], df3['lower-band'] = talib.BBANDS(closes, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "# 2.3 %B %保力加\n",
    "df3['%b'] = (df3['Close']-df3['lower-band'])/(df3['upper-band']-df3['lower-band'])*100\n",
    "df3['%b-high']  = common.percentB_belowzero(df3['%b'], df3['Close']) \n",
    "df3['%b-low'] = common.percentB_aboveone(df3['%b'], df3['Close'])\n",
    "# 2.4 OBV 能量潮\n",
    "df3['obv'] = talib.OBV(closes, vols)\n",
    "# 2.5 VOL EMA\n",
    "df3['vol-ema5'] = talib.EMA(vols, timeperiod=5)\n",
    "# 2.6 P-SAR 抛物线\n",
    "df3['p-sar'] = talib.SAR(highs, lows, acceleration=0.02, maximum=0.2)\n",
    "# 2.7 VWAP 成交量加權平均價格\n",
    "period = [5, 40, 80, 120]\n",
    "for v in period:\n",
    "    df3['typical-price'] = (df3['High'] + df3['Low'] + df3['Close']) / 3\n",
    "    df3['turnover'] = df3['typical-price'] * df3['Volume']\n",
    "    df3['cum-turnover-'+str(v)] = df3['turnover'].rolling(window=v).sum()\n",
    "    df3['cum-volume-'+str(v)] = df3['Volume'].rolling(window=v).sum()\n",
    "    df3['vwap-'+str(v)] = df3['cum-turnover-'+str(v)] / df3['cum-volume-'+str(v)]\n",
    "    df3['vwap-'+str(v)] = df3['vwap-'+str(v)].replace([np.inf, -np.inf], 0)\n",
    "    df3['vwap-'+str(v)].fillna(0, inplace=True)\n",
    "    drop_list_1 = ['turnover', 'typical-price', 'cum-turnover-'+str(v), 'cum-volume-'+str(v)]\n",
    "    df3.drop(drop_list_1, axis=1, inplace=True)\n",
    "# 2.8 MACD\n",
    "df3['macd'], df3['macdsignal'], df3['macdhist'] = talib.MACD(closes, fastperiod=12, slowperiod=26, signalperiod=9*40)\n",
    "# 2.9 KDJ\n",
    "df3['k-kdj'], df3['d-kdj'], df3['j-kdj'] = common.kdj(highs, lows, closes, window_size=20)\n",
    "df3['diff-kdj'] = df3['k-kdj']-df3['d-kdj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 separate to daily\n",
    "data = {}\n",
    "days1 = list(dict.fromkeys([v.date() for v in df3['udate']])) # how many tradeing day\n",
    "for day in days1:\n",
    "    day_start = datetime(day.year, day.month, day.day, 5, 0, 0)\n",
    "    day2 = day + timedelta(days=1)\n",
    "    day_end = datetime(day2.year, day2.month, day2.day, 3, 55, 0)\n",
    "    mask = ((df3['udate'] >= day_start) & (df3['udate'] <= day_end))\n",
    "    df4 = df3.loc[mask]\n",
    "    if (df4.shape[0] > 1):\n",
    "        data[day] = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 draw chart\n",
    "drop_list_2 = []\n",
    "for k, _df5 in data.items():\n",
    "    df5 = _df5.copy(deep=True)\n",
    "    # 4.1 style\n",
    "    style = mpf.make_mpf_style(base_mpf_style='charles', rc={'font.size':6})\n",
    "    # 4.2 addplot\n",
    "    apds = [mpf.make_addplot(df5[['lower-band','upper-band']],panel=0,color='orange',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['vwap-40'].replace(0, np.nan),panel=0,color='aqua',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['%b-low'],type='scatter',markersize=20,marker='v',panel=0),\n",
    "            mpf.make_addplot(df5['%b-high'],type='scatter',markersize=20,marker='^',panel=0),\n",
    "            mpf.make_addplot(df5['p-sar'],scatter=True,markersize=1,marker='*',panel=0,color='blueviolet'),\n",
    "            #\n",
    "            mpf.make_addplot(df5['vol-ema5'],panel=1,color='orange'),\n",
    "            #\n",
    "            mpf.make_addplot(df5['macd'],panel=2,color='orange'),\n",
    "            mpf.make_addplot(df5['macdsignal'],panel=2,color='violet'),\n",
    "            mpf.make_addplot(df5['macdhist'],panel=2,type='bar',color='dimgray'),\n",
    "            #\n",
    "            mpf.make_addplot(df5['k-kdj'],panel=3,color='orange'),\n",
    "            mpf.make_addplot(df5['d-kdj'],panel=3,color='violet'),\n",
    "            mpf.make_addplot(df5['j-kdj'],panel=3,color='aqua'),\n",
    "            mpf.make_addplot(df5['diff-kdj'],panel=3,type='bar',color='dimgray')]\n",
    "    # 4.3 render\n",
    "    if (False):\n",
    "        try:\n",
    "            print(df5.shape, df5['udate'].iloc[0], df5['udate'].iloc[-1])\n",
    "            mpf.plot(df5, type='candle', addplot=apds, style=style, ylabel='', ylabel_lower='', volume=True, figscale=0.5, xrotation=0, datetime_format=\"%H:%M\", show_nontrading=False, tight_layout=True, savefig='./data/img-nq/features/'+k.strftime('%m-%d-%Y')) \n",
    "        except:\n",
    "            drop_list_2.append(k)\n",
    "    # 4.4 drop err day\n",
    "    elif df5['Volume'].shape[0] == df5['Volume'].isin([0]).sum():\n",
    "        drop_list_2.append(k)\n",
    "\n",
    "# ERROR: 10-29 & 10-30\n",
    "for k in drop_list_2:\n",
    "    data.pop(k, None)\n",
    "    print('excpet & delete: ', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集: Total dataset has 88966 samples, and 15 features.\n",
      "分包: Train Data: (54450, 15), Test Data: (17572, 15), Validation Data: (16744, 15)\n",
      "訓練集: X_train Data: (54403, 40, 15), Y_train Data: (54403, 1)\n",
      "測試集: X_Test Data: (17525, 40, 15), Y_Test Data: (17525, 1)\n",
      "验证集: X_Valid Data: (16697, 40, 15), Y_Valid Data: (16697, 1)\n"
     ]
    }
   ],
   "source": [
    "# 5.0 合併\n",
    "df7 = pd.DataFrame()\n",
    "for k, df6 in data.items():\n",
    "    df7 = pd.concat([df7, df6], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "\n",
    "# 5.1 清洗\n",
    "df8 = df7.copy(deep=True)\n",
    "drop_list_3 = ['udate', 'High', 'Low', 'Open', 'Volume', 'obv', 'sma-5', 'sma-80', 'vwap-5', 'vwap-80', 'macd', 'macdsignal', \n",
    "               'middle-band', 'k-kdj', 'd-kdj']\n",
    "df8.drop(drop_list_3, axis=1, inplace=True)\n",
    "df8.fillna(0, inplace=True)\n",
    "df8 = df8.round(2)\n",
    "\n",
    "# 5.2 檢查\n",
    "is_contain_null = df8.isnull().sum()\n",
    "is_contain_nan = df8.isna().sum()\n",
    "is_contain_inf = df8.isin([np.nan]).sum()\n",
    "print('数据集: Total dataset has {} samples, and {} features.'.format(df8.shape[0], df8.shape[1])) # df8.info()\n",
    "    \n",
    "# 5.4 儲存\n",
    "path_data = os.path.abspath(os.path.join('data', 'nq', 'nq-clean-data-with-features.csv'))\n",
    "if os.path.exists(path_data):\n",
    "    os.remove(path_data)\n",
    "df8.to_csv(path_data)\n",
    "\n",
    "# 5.3 正則化\n",
    "min_max_scaler, standar_scaler = MinMaxScaler(feature_range=(-0.99, 0.99)), StandardScaler()\n",
    "_df8 = min_max_scaler.fit_transform(df8)\n",
    "df8 = pd.DataFrame(_df8, columns=df8.columns, index=df8.index)\n",
    "\n",
    "# 5.5 分包\n",
    "no_day = np.array([v.shape[0] for k, v in data.items()])\n",
    "no_day_1, no_day_2, no_day_3 = no_day[:-30].sum(), no_day[:-15].sum(), no_day.sum()\n",
    "train_data = df8.iloc[200 : no_day_1]\n",
    "test_data = df8.iloc[no_day_1 : no_day_2]\n",
    "validation_data = df8.iloc[no_day_2 : no_day_3]\n",
    "print('分包: Train Data: {}, Test Data: {}, Validation Data: {}'.format(train_data.shape, test_data.shape, validation_data.shape))\n",
    "\n",
    "# 5.6 輸入層數據\n",
    "def split_data_v2(data, timesteps, t_pus_no):\n",
    "    x_data, y_data = [], []\n",
    "    no_max = data.shape[0]-t_pus_no\n",
    "    for i in range(timesteps, no_max):\n",
    "        start, end = i-timesteps, i\n",
    "        temp_0 = data.iloc[end: end+t_pus_no]\n",
    "        y_data.append(temp_0['Close'].tail(1))\n",
    "        temp_1 = data.iloc[start: end]\n",
    "        x_data.append(temp_1)\n",
    "    x_data, y_data = np.array(x_data), np.array(y_data)\n",
    "    return [x_data, y_data]\n",
    "\n",
    "# 5.7 窗口步長\n",
    "t_pus_no = 7\n",
    "window_size_1 = 40\n",
    "window_size_2 = 40\n",
    "x_train, y_train = split_data_v2(train_data, window_size_1, t_pus_no)\n",
    "x_test, y_test = split_data_v2(test_data, window_size_1, t_pus_no)\n",
    "x_valid, y_valid = split_data_v2(validation_data, window_size_2, t_pus_no)\n",
    "\n",
    "# no_batches, timesteps, no_features\n",
    "print('訓練集: X_train Data: {}, Y_train Data: {}'.format(x_train.shape, y_train.shape))\n",
    "print('測試集: X_Test Data: {}, Y_Test Data: {}'.format(x_test.shape, y_test.shape))\n",
    "print('验证集: X_Valid Data: {}, Y_Valid Data: {}'.format(x_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 40, 1024)          4259840   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 40, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 29,437,952\n",
      "Trainable params: 29,437,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "1701/1701 [==============================] - 111s 65ms/step - loss: 0.0813 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "1592/1701 [===========================>..] - ETA: 6s - loss: 0.0193 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# 6.1.1 模型参数\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "units = 1024\n",
    "verbose = 1\n",
    "no_batches = x_train.shape[0]\n",
    "timesteps = x_train.shape[1]\n",
    "no_features = x_train.shape[2]\n",
    "batch_input_shape = (timesteps, no_features)\n",
    "\n",
    "# 6.1.2 日志参数\n",
    "prefix = 'nq-lstm'\n",
    "cur_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 6.2 模型\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=units, recurrent_activation='sigmoid', activation='tanh', unroll=False, use_bias=True, \n",
    "                               recurrent_dropout=0, return_sequences=True, input_shape=batch_input_shape))\n",
    "model.add(Dropout(rate=0.20))\n",
    "model.add(tf.keras.layers.LSTM(units=units, return_sequences=True, input_shape=batch_input_shape))\n",
    "model.add(Dropout(rate=0.15))\n",
    "model.add(tf.keras.layers.LSTM(units=units, return_sequences=True, input_shape=batch_input_shape))\n",
    "model.add(Dropout(rate=0.10))\n",
    "model.add(tf.keras.layers.LSTM(units=units, return_sequences=False, input_shape=batch_input_shape))\n",
    "model.add(Dropout(rate=0.05))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 6.3.1 check point\n",
    "checkpoint_dir = './training_checkpoints/'+ prefix +'-' + cur_time\n",
    "os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "# 6.3.2 tensor board\n",
    "log_dir = os.path.join('./logs/fit/'+ prefix +'-') + cur_time\n",
    "os.mkdir(log_dir)\n",
    "tensor_board_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# 6.4 fit model\n",
    "history_model = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True,\n",
    "                          verbose=verbose, callbacks=[checkpoint_callback, tensor_board_callback])\n",
    "\n",
    "# 6.5 save model\n",
    "model_path = \"./saved_model/\"+prefix+\"-\"+cur_time+\".h5\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 visualize loss\n",
    "keys = list(history_model.history.keys())\n",
    "training_loss = history_model.history['loss']\n",
    "test_loss = history_model.history['val_loss']\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('model:'+prefix+'   batch_size:'+str(batch_size)+'   epochs:'+str(epochs)+'   units:'+str(units),loc ='left')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.savefig('./data/img-nq/results/'+cur_time+'-loss')\n",
    "plt.clf()\n",
    "\n",
    "# 7.2 visualize accuracy\n",
    "training_accuracy = history_model.history['accuracy']\n",
    "test_accuracy = history_model.history['val_accuracy']\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('model:'+prefix+'   batch_size:'+str(batch_size)+'   epochs:'+str(epochs)+'   units:'+str(units),loc ='left')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.savefig('./data/img-nq/results/'+cur_time+'-accuracy')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 evaluate\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train Score: %.4f MSE (%.4f RMSE)' % (train_score[0], math.sqrt(train_score[0])))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Score: %.4f MSE (%.4f RMSE)' % (test_score[0], math.sqrt(test_score[0])))\n",
    "valid_score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "print('Validate Score: %.4f MSE (%.4f RMSE)' % (valid_score[0], math.sqrt(valid_score[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0 prediction\n",
    "predict1 = model.predict(x_valid)\n",
    "\n",
    "# 9.1 inverse\n",
    "len_shape_y = x_valid.shape[2]-1\n",
    "fill_list = list(repeat(0, len_shape_y))\n",
    "predict2 = [[v[0]] + fill_list for v in predict1]\n",
    "close2 = [[v[0]] + fill_list for v in y_valid]\n",
    "\n",
    "predict3 = min_max_scaler.inverse_transform(predict2)\n",
    "close3 = min_max_scaler.inverse_transform(close2)\n",
    "\n",
    "predict4 = [v[0] for v in predict3]\n",
    "close4 = [v[0] for v in close3]\n",
    "\n",
    "# 9.3 % change\n",
    "df9 = pd.DataFrame({'predict_price': predict4, 'close_price': close4})\n",
    "df9.index = validation_data.tail(df9.shape[0]).index\n",
    "df10 = df9.pct_change(fill_method ='ffill')\n",
    "df10 = df10.rename(columns={\"predict_price\": \"predict_preice_pct\", \"close_price\": \"close_price_pct\"})\n",
    "df10 = df10.round(8)\n",
    "df11 = pd.concat([df10, df9], axis=1)\n",
    "\n",
    "# 9.4 save\n",
    "path_data = os.path.abspath(os.path.join('data', 'nq', 'nq-prediction.csv'))\n",
    "if os.path.exists(path_data):\n",
    "    os.remove(path_data)\n",
    "df11.to_csv(path_data)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.set_size_inches(12.5, 6.5)\n",
    "axs[0].plot(df11.index, df11['predict_price'])\n",
    "axs[1].plot(df11.index, df11['close_price'])\n",
    "axs[0].grid(True)\n",
    "axs[1].grid(True)\n",
    "axs[0].set_title('predict price')\n",
    "axs[1].set_title('close price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
