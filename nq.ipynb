{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import talib as talib\n",
    "import numpy as np\n",
    "import data as ds\n",
    "import common as common\n",
    "import os as os\n",
    "import datetime as datetime\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设定\n",
    "pd.options.mode.chained_assignment = None\n",
    "# 不让程序占满 GPU 内存\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = os.path.abspath(os.path.join('data', 'nq', '20201020_064100.csv'))\n",
    "file_2 = os.path.abspath(os.path.join('data', 'nq', '20201023_064200.csv'))\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chng obtains 0\n",
      "pchng obtains 0\n"
     ]
    }
   ],
   "source": [
    "# 1.0 reshapre dataframe 重構數據集\n",
    "df1.index = pd.to_datetime(df1.stime)\n",
    "df2.index = pd.to_datetime(df2.stime)\n",
    "df3 = pd.concat([df1, df2], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "df3 = df3.rename(columns={\"high\": \"High\", \"low\": \"Low\", \"open\": \"Open\", \"last\": \"Close\", \"vol\": \"Accumulated Volume\"})\n",
    "df3['udate'] = pd.to_datetime(df3['udate'])\n",
    "df3['udate2'] = [v.date() for v in df3['udate'].tolist()]\n",
    "df3['udate2'] = pd.to_datetime(df3['udate2'])\n",
    "df3['interactive_udate'] = pd.to_datetime(df3['interactive_udate'])\n",
    "df3['mdate'] = pd.to_datetime(df3['mdate'])\n",
    "df3['stime'] = pd.to_datetime(df3['stime'])\n",
    "# 1.1 data validation 數據有效性檢查\n",
    "shape = df3.shape\n",
    "types = df3.dtypes\n",
    "types1 = {'High': 'float64', 'Low': 'float64', 'Open': 'float64', 'Close': 'float64', 'Accumulated Volume': 'int64', 'chng': 'float64', 'pchng': 'float64'}\n",
    "df3.astype(types1).dtypes\n",
    "# 1.2 zero / nan\n",
    "for k, v in types1.items():\n",
    "    if (df3[k].isin([np.nan]).any().any()):\n",
    "        print(k+' obtains nan')\n",
    "    if (df3[k].isin([0]).any().any()):\n",
    "        print(k+' obtains 0')\n",
    "# 1.3 overview\n",
    "is_contain_nan = df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 technical indicator\n",
    "highs = np.array(df3['High'], dtype='float')\n",
    "lows = np.array(df3['Low'], dtype='float')\n",
    "opens = np.array(df3['Open'], dtype='float')\n",
    "closes = np.array(df3['Close'], dtype='float')\n",
    "# 2.1 SMA 均線\n",
    "for v in [5, 10, 20, 50, 80, 100, 120, 150, 180, 200]:\n",
    "    df3['sma-'+str(v)]= talib.SMA(closes, timeperiod=v)\n",
    "# 2.2 Bollinger 保力加\n",
    "df3['upper-band'], df3['middle-band'], df3['lower-band'] = talib.BBANDS(closes, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "# 2.3 %B %保力加\n",
    "df3['%b'] = (df3['Close']-df3['lower-band'])/(df3['upper-band']-df3['lower-band'])*100\n",
    "# 2.4 P-SAR 抛物线\n",
    "df3['p-sar'] = talib.SAR(highs, lows, acceleration=0.02, maximum=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 separate to daily data set\n",
    "data = {}\n",
    "days1 = list(dict.fromkeys(df3['udate2'].tolist())) # how many tradeing day\n",
    "for day in days1:\n",
    "    day_start = datetime(day.year, day.month, day.day, 6, 0, 0)\n",
    "    day2 = day + timedelta(days=1)\n",
    "    day_end = datetime(day2.year, day2.month, day2.day, 6, 0, 0)\n",
    "    mask = ((df3['udate'] >= day_start) & (df3['udate'] <= day_end))\n",
    "    df4 = df3.loc[mask]\n",
    "    if (df4.shape[0] > 1):\n",
    "        # 3.1 calculate vol by minute\n",
    "        df4['Volume'] = df4['Accumulated Volume'].diff()\n",
    "        # 3.2 OBV 能量潮\n",
    "        closes = np.array(df4['Close'], dtype='float')\n",
    "        vols = np.array(df4['Volume'], dtype='float')\n",
    "        df4['obv'] = talib.OBV(closes, vols)\n",
    "        # 3.3 vol EMA\n",
    "        df4['vol-sma5'] = talib.EMA(vols, timeperiod=5)\n",
    "        # 3.4 %b\n",
    "        df4['%b-high']  = common.percentB_belowzero(df4['%b'], df4['Close']) \n",
    "        df4['%b-low'] = common.percentB_aboveone(df4['%b'], df4['Close'])\n",
    "        # 3.5 assgin to data\n",
    "        data[day] = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 draw chart\n",
    "for k, df5 in data.items():\n",
    "    # 4.1 style\n",
    "    style = mpf.make_mpf_style(base_mpf_style='charles', rc={'font.size':6})\n",
    "    # 4.2 addplot\n",
    "    apds = [mpf.make_addplot(df5[['lower-band','upper-band']],panel=0,color='orange',linestyle='dashdot'),\n",
    "            mpf.make_addplot(df5['%b-low'],type='scatter',markersize=20,marker='v'),\n",
    "            mpf.make_addplot(df5['%b-high'],type='scatter',markersize=20,marker='^'),\n",
    "            mpf.make_addplot(df5['p-sar'],scatter=True,markersize=1,marker='*',panel=0,color='blueviolet'),\n",
    "            mpf.make_addplot((df5['vol-sma5']),panel=1,color='orange')]\n",
    "    # 4.3 render\n",
    "    if (False):\n",
    "        print(df5.shape, df5['udate'].iloc[0], df5['udate'].iloc[-1])\n",
    "        mpf.plot(df5, type='line', style=style, ylabel='', ylabel_lower='', volume=True, figscale=0.5, xrotation=0, datetime_format=\"%H:%M\", show_nontrading=False, tight_layout=True, addplot=apds) # savefig='./data/img-nq/'+k.strftime('%m-%d-%Y') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset has 13268 samples, and 26 features.\n",
      "Train Data: (12744, 26), Test Data: (523, 26)\n",
      "X_train Data: (12744, 26, 1), Y_train Data: (12744,)\n",
      "X_Test Data: (523, 26, 1), Y_Test Data: (523,)\n"
     ]
    }
   ],
   "source": [
    "# 5.0 合併\n",
    "df7 = pd.DataFrame()\n",
    "for k, df6 in data.items():\n",
    "    df7 = pd.concat([df6, df7], axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "# 5.1 清洗\n",
    "df8 = df7.copy(deep=True)\n",
    "drop_list = ['udate', 'code', 'name', 'nmll', 'bid', 'ask', 'bsize', 'asize', 'clast', 'Accumulated Volume', 'turnover', \n",
    "             'currency',  'yrhigh', 'yrlow', 'tnover_sc', 'lotsize', 'calllv', 'issued', 'ichng', 'ucode', 'ratio', 'strike', 'mdate', \n",
    "             'issuer', 'traded', 'ocode', 'eikon_udate', 'interactive_udate', 'instrument_type', 'stime', 'udate2']\n",
    "df8.drop(drop_list, axis=1, inplace=True)\n",
    "df8.fillna(0, inplace=True)\n",
    "is_contain_null = df8.isnull().sum()\n",
    "is_contain_nan = df8.isna().sum()\n",
    "print('Total dataset has {} samples, and {} features.'.format(df8.shape[0], df8.shape[1])) # df8.info()\n",
    "# 5.2 分包\n",
    "train_data = df8.iloc[0 : 12744]\n",
    "test_data = df8.iloc[12745 : 13268]\n",
    "print('Train Data: {}, Test Data: {}'.format(train_data.shape, test_data.shape))\n",
    "# 5.3 模型数据\n",
    "# 5.3.1 train data\n",
    "x_train, y_train = [], []\n",
    "for k, v in train_data.iterrows():\n",
    "    x_train.append(v.tolist())\n",
    "    y_train.append(v['Close'])\n",
    "x_train, y_train =  np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],  x_train.shape[1],  1))\n",
    "print('X_train Data: {}, Y_train Data: {}'.format(x_train.shape, y_train.shape))\n",
    "# 5.3.2 test data\n",
    "x_test, y_test = [], []\n",
    "for k, v in test_data.iterrows():\n",
    "    x_test.append(v.tolist())\n",
    "    y_test.append(v['Close'])\n",
    "x_test, y_test =  np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],  x_test.shape[1],  1))\n",
    "print('X_Test Data: {}, Y_Test Data: {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 26, 1000)          4008000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 26, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 26, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 26, 1000)          8004000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 1000)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 26, 1)             1001      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 26, 1)             0         \n",
      "=================================================================\n",
      "Total params: 28,021,001\n",
      "Trainable params: 28,021,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74821.2031 - accuracy: 0.0164\n",
      "Epoch 2/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75177.9609 - accuracy: 0.0160\n",
      "Epoch 3/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75327.2344 - accuracy: 0.0159\n",
      "Epoch 4/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74923.8906 - accuracy: 0.0160\n",
      "Epoch 5/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74724.5469 - accuracy: 0.0159\n",
      "Epoch 6/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75022.5938 - accuracy: 0.0160\n",
      "Epoch 7/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74705.1562 - accuracy: 0.0158\n",
      "Epoch 8/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75214.2109 - accuracy: 0.0159\n",
      "Epoch 9/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75165.9609 - accuracy: 0.0159\n",
      "Epoch 10/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75040.5391 - accuracy: 0.0159\n",
      "Epoch 11/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74667.8906 - accuracy: 0.0159\n",
      "Epoch 12/20\n",
      "399/399 [==============================] - 17s 44ms/step - loss: -75198.6016 - accuracy: 0.0159\n",
      "Epoch 13/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -75330.5781 - accuracy: 0.0157\n",
      "Epoch 14/20\n",
      "399/399 [==============================] - 17s 44ms/step - loss: -75222.4219 - accuracy: 0.0158\n",
      "Epoch 15/20\n",
      "399/399 [==============================] - 17s 44ms/step - loss: -74903.3047 - accuracy: 0.0156\n",
      "Epoch 16/20\n",
      "399/399 [==============================] - 17s 44ms/step - loss: -75179.2266 - accuracy: 0.0159\n",
      "Epoch 17/20\n",
      "399/399 [==============================] - 17s 44ms/step - loss: -75286.5234 - accuracy: 0.0158\n",
      "Epoch 18/20\n",
      "399/399 [==============================] - 17s 44ms/step - loss: -74927.8359 - accuracy: 0.0159\n",
      "Epoch 19/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74678.3750 - accuracy: 0.0159\n",
      "Epoch 20/20\n",
      "399/399 [==============================] - 17s 43ms/step - loss: -74854.1250 - accuracy: 0.0161\n"
     ]
    }
   ],
   "source": [
    "# 6.0 模型参数\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "units = 100\n",
    "cur_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 6.1 模型\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(tf.keras.layers.Dense(units=1))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n",
    "# 6.2.1 創建check point\n",
    "checkpoint_dir = './training_checkpoints/nq-lstm-' + cur_time\n",
    "os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "# 6.2.2 創建tensor board\n",
    "log_dir = os.path.join('./logs/fit/nq-lstm-') + cur_time\n",
    "os.mkdir(log_dir)\n",
    "tensor_board_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# 6.3 fit model\n",
    "history = model_lstm.fit(x_train, x_train, epochs = epochs, batch_size = batch_size, \n",
    "                         callbacks=[checkpoint_callback, tensor_board_callback])\n",
    "\n",
    "# 6.4 儲存模型\n",
    "model_path = \"./saved_model/nq_lstm_\"+cur_time+\".h5\"\n",
    "model_lstm.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
